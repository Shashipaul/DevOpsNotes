Node : 
     Kube proxy
	 kubectl
	 kubeadm
	 
Master :
	Kube Apiservice
	Kube etcd
	kube Controller
	kube Scheduler
	 
	 
	  



kind : pod
------------------------------------------------------------------------------------------
Pod Controller :
	kind : ReplicationController
	kind : ReplicaSet
	kind : Deployment (RS+ rolling Update + rollingout feature)
	

Cluster IP: Service accessable withen clister but not from outside netowrk (.i.e Database)
Node port service : Application accessable from outside netowrk (i.e. Web Applications) 

kubeadm init --tocken-ttl 0 ---->tocken will not expire


kubeadm init --tocken-ttl=0 --apiserver-advertise-adress=<<apiserver>>   ------> Bind apiserver


kubectl get nodes


kubeadm token create --print-join-command ---------> create Join command

example: 
kubeadm join 192.168.18.27:6443 --token 8z5git.3jssximhbk36h2y7 --discovery-token-ca-cert-hash sha256:518a401af9f969ea5d6322f3a308a9b44af6bacd9bbd29dd532c566cb8eba113

------Network Configuration for pod/containers------

1) two pod can communicate each other over the same node 
2) two pod can communicate over the different nodes
3) All pod will get uniq ipaddress for communication

Software define network(SDN)
CNI (Contaianer network interface)
1) Weave
2) Calico
3) Flannel
4) OVS

https://v1-17.docs.kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/
https://v1-17.docs.kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/

ip a s weave --------------------------------> Check the details of the master overlay swtich for (SDN) 


kubectl run test --image=nginx -----------------------> (create pod for nginx) --> execute on master node

kubectl get pod -o wide   ---------> Check the status of pod on master node

kubectl get ds -n            -----------> check for the SDN switches
  
kubectl get ds -n kube-system   -----------> check for the kube SDN switches

kubectl get po -n kube-system -o wide | grep weave 

kubectl describe ds weave-net -n kube-system | grep -i image   -----> detail for cluster switch

------etcd----> maintain information about the subnets 


--------------------------------------------------->How to deploy application in kubernetes<-------------------------------------------------------

------------------------>one pod having multiple containers<---------------------------------



1) Command line : kubectl 
2) File method  : 
			lab1(Master Node): how to create plane pod
					kubectl create ns qa   ----->create namespace for QA team. 
					kubectl get ns ------> list namespace
					kubectl run test --image=nginx  ------>  create pod in default namespace Command line
					kubectl run test --image=nginx -n qa  ------>  create pod in qa namespace from Command line
					kubectl get pod -n default -----> list running pods in default namespace
					kubectl get pod -n qa -----> list running pods in qa namespace
					kubectl delete pod test  ----> deleting pod test
					kubectl delete pod test -n qa ----> deleting pod test under the namespace qa
					kubectl get po -n qa   -----> list for the pods under qa
					kubectl get po -n qa -i -o wide ----> list for the node where pod is running
					kubectl run test --image=nginx -n qa --dry-un -o yaml   ----> backend file create for pod
					kubectl run test --image=nginx -n qa --dry-run -o yaml > pod.yml ----> backend file create for pod and write into pod.yml file
					kubectl run test --image=nginx -n qa --dry-run -o json > pod.json ----> backend file create for pod and write into pod.json file
					kubectl delete ns qa ----> delete all the pods running under the namesapece qa
     Pod creation from yaml (file method) based:
					kubectl create -f pod.yml -n qa
     Check the logs file :
					kubectl logs 
					
----------------------------------------------Replication Controller--------------------------------------------------------						
How to create pod using Kubernetes controller Resources
					1) RC : Replication Controller   (Pod management deployment process)
						Running an example ReplicationController 
						yaml file for ReplicationController pod creation 
						Refrence Page : https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/
						
-----------------------------------------------ReplicationController.yml-------------------------------------------------------						
								apiVersion: v1
								kind: ReplicationController                                               
								metadata:
									name: nginx
								spec:
									replicas: 3
									selector:
										app: nginx
									template:
										metadata:
											name: nginx
											labels:
												app: nginx
									spec:
										containers:
											- name: nginx
											  image: nginx
											  ports:
												- containerPort: 80
------------------------------------------------------------------------------------------------------		
			
		    kubectl get po ----------> check for the pods created by  ReplicationController 
		    kubectl describe pod <pod-name> | less   -----> describe the pod details
			kubectl get rc -------> check for existing resources
			kubectl scale --replicas=8 rc <rc-name> ---------> create replicas of existing pods (manual Scalling in and Scalling out)
			kubectl get po -o wide    ------> list the running pods details
			
			
			kubectl edit rc <rc-name> ----------> check for the file where got information for running rc .i.e etcd file 
			
			kubectl apply -f ReplicationController.yml ------> push for rc change forcefully
			kubectl delete rc <rc-name> -------> delete ReplicationController
			
-------------------------------------------------->REPLICASET<--------------------------------------------------------			

-----------------------------------------------replicaset.yml-------------------------------------------------------		
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # modify replicas according to your case
  replicas: 3
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google_samples/gb-frontend:v3	
---------------------------------------------------------------------------------------------------------	
		ReplicaSet help : high availability / state maintain / scale ability / pod down
		
					kubectl apply -f replicaset.yml ----------------------> create pod for new replicasert
					kubectl get rs
					kubectl get pods
					kubectl describe pod <pod-name> | grep -i controlled   -----> describe the pod details
					
					
---------------------------------------------------------------------------------------------------------
rolling update / rollout strategy

				Deployment : 
					RS+ rolling Update + rollingout feature
					
Day03	
-------------------------------------------------->Networking (Day03)<--------------------------------------------------------
				
  Unit : Networking
				1) Node Network and Pod Network Isolate
				2) Pod should be communicate with same host using provate IPs
				3) Pod should be communicate on the remote host Node using provate IPs
				4) Pod should be access external Network
				5) Traffice should be distribute across the application Pod (Pod lavel balancer)											
				6) External User should be access your application which into kubernetes cluster
				
			statements already avalible in Pod network
				note: kubernetes by default provide software Load balancer over the Node lavel 
							service: kube-proxy: 

							
							
		--------------------Creating Software Load Balancer(Traffice should be distribute across the application Pod (Pod lavel balancer))-------------------------
						Service IP Create
							Add value in yml file ----> (kind: Service, name: my-service, env: prod) for creating load balancer
							kubectl create -f service.yml    -----> creating load balancer
							kubectl get service              -----> list existing services
							kubectl describe service my-service
							kubectl get ds -n kube-system
							kubectl create deploy test --image=nginx
							kubectl delete all -l app=test
							kubectl get po -o wide 
							kubectl exec -it pod1 /bin/sh
							kubectl get po --show-labels
							kubectl run pod1 --image=nginx --labels=env=prod 
							kubectl run pod2 --image=nginx --labels=env=prod          
	
	
	
	-----Create service ip from command line:
				kubectl scale --replicas=4 deploy test --> create 4 replicas
				kubectl expose deploy test --port=8080 --target-port=80	
     Check for the selector value :
				kubectl describe service test | grep -i -B2 selector
				
	--------------------External User should be access your application which into kubernetes cluster-------------------------	
		
				kubectl expose deploy test --port=8080 --target-port=80	--type=NodePort	

								----->External Load Balancer<-------	
									
							node 1 (Expose NodePort 31247) 
								internal load balancer (Kube Proxy)----->External Load Balancer (3 party) -----> DNS ()
							node 2 (Expose NodePort 31247)
							
							
							
							Creating External Load Balancer on AWS 
							Under the EC2----> Select  Load balancers----> Create Load balancer 
																							---> Application Load Balancer
																														--> Target Group 
																																	-> Name (kubernetes_target)
																														--> Register Targets
																																    -> Add Servers (Select both instances then on port change port Then add to registered)
							Use DNS name for further use of configuration																										
																																	
 
Day04:
-------------------------------------------------->Service Discovery(Day04)<--------------------------------------------------------
kubectl service po -n kube-system
kubectl get pod
kubectl get deploy
kubectl delete deploy test
kubectl get all
kubectl create -f pod.yml 

kubectl get po -o wide | grep -i running

kubectl get service -n kube-system
nslookup <<Dns Name>>>

kubectl create deploy webapp --image=nginx

kubectl expose deploy webapp --port=80


mysql--pod--service-1:CLUSTER   ----> tier1

wordpress--pod--service-1:NodePort ----> tier2


      --------------------How to write yml file-------------------------
	  
		kubectl explain deploy
		kubectl explain deploy.metadata
		kubectl explain deploy.specs
		kubectl explain deploy.specs.selector		
		
	  -----------------------Create mysql pod----------------------------
	 Refrence:  https://kubernetes.io/docs/tutorials/stateful-application/mysql-wordpress-persistent-volume/   
	  kubectl create -f mysql_pod.yml
	  kubectl expose deploy mysql --port=3306
	  
	  	  -----------------------Create wordpress pod----------------------------
	 Refrence:  https://kubernetes.io/docs/tutorials/stateful-application/mysql-wordpress-persistent-volume/   
	  kubectl create -f wordpress-pod.yml
	  kubectl expose deploy mysql wordpress --type=NodePort
	  kubectl get service wordpress
	  kubectl describe service  wordpress
	  
	  --------------------Pod Scheduling-------------------------
	  
	  Type 1) Node Based	  
	  
		kubectl run pod1 --image=nginx --dry-run -o yaml > pod.yml
		kubectl create -f Scheduling-Node-Based-pod.yml
		kubectl get po pod1 -o wide
	  
		vim deployment-Based-pod.yml
		kubectl create -f deployment-Based-pod.yml
		kubectl get po pod1 -o wide
	  
	  Type 2) lable and selector
	  
		kubectl label node node1.example.com disk=ssd                  ----> Assign lables to node
		kubectl label node node2.example.com disk=hdd                  ----> Assign lables to node
		kubectl get node -L disk                               ---------------> list all the pods with the label disk
		kubectl get node node1.example.com --show-labels         ---------------> list all the labels
		kubectl edit node node1.example.com  --------> How to check labels in etcd
		kubectl delete pod pod1     -----> delete existing pods
		kubectl delete deploy test      -----> delete deployment test
		
		Note : Check for the disk tag under the nameSector
				
		kubectl create -f deployment-Based-pod_lable_selector.yml
		kubectl get po pod1 -o wide   ----> list pod details
		kubectl get events | grep -i falied    ---> check for the event recorded with falied
		kubectl label node node1.example.com disk=sata --overwrite ---> use to change the node lable 
		
		
Day05:		
--------------------Pod Scheduling-------------------------		
		
	  Type 3) Node Affinity and anti-affinity
	  
			kubectl lable node worker1.example.com disk=ssd
			kubectl create -f Scheduling-Affinity-anti-affinity-node.yml
			
			rule:
				soft: perfered
				hard: required
			
			kubectl get node -L disk
			kubectl get po with-node-affinity -o wide
			
		operator:
		   Affinity:
				In
				Exists
		   Anti-affinity:
				NotIn
				DoesNotExists
		 
			kubectl get po with-node-affinity -o wide with-node-affinity
		
	  
	  
		Type 4) Pod Affinity and anti-affinity   (Particulars type of pod not scheduled on same nodes)
		
			kubectl get all
			
			kubectl create -f Scheduling-Affinity-anti-affinity-pod.yml
			
			kubectl create -f Scheduling-Affinity-anti-affinity-pod-app.yml  (Create simple pod for app tag)
			
			
			kubectl create -f Scheduling-anti-affinity-pod.yml  (not to create Database pod where app pod is avalible)
			
			| Node1 		| Node2 		| Node3
			| webserver1    | webserver2    | webserver3
			| cache-1		| cache-2		| cache-3	
			
			kubectl create -f redis-podAntiAffinity-pod.yml
			kubectl create -f  web-server-podAntiAffinity-pod.yml  (rule is not to run same pods on same nodes)
			
			kubectl scale --replicas=3 deploy redis-cache    ---> Increase the scaleblity of redis-cache server
		

			kubectl create -f podAntiAffinity_podAffinity_pod.yml   -----> (it has a rule to run web and redis on same node but both have independent on everynode)
			kubectl get po -l app=store 
			
			kubectl describe node <node name> | less   ------------> Check for the topologyKey its depends on service provider .i.e AWS(EKS)
			kubectl get events
			
		Type 5) taint and toleration		
			 how to check of node is taint or not
			      kubectl describe nodes <nodename> | grep -i taint
			  command to create node taint
				kubectl taint node <node name> key=value:Effect
				example : kubectl taint nodes worker1.emaple.com red=color:NoSchedule
						example of key=value is red=color
						Effect (3 type) :  1. NoSchedule
										   2. NoExecute        --> dont on existing pod which already without toleration
										   3. PreferNoSchedule  ---> soft
										   
			Create Pod with tolerations :
				kubectl create -f taint_toleration_pod.yml	
            kubectl get po -all-namesapeces -o wide | greap worker1 				
				
			kubectl get po -all-namesapeces -o wide | greap worker1 | wc -l ------> count
			
			
			
Day06:		
---------------------------------------------Volume(Day06)-----------------------------------------------------------------
Type of Volume:
	1) Shared
	2) Non Shared

How to provide external volume for Pod
Volume controller resources

			1) PV: Persistent Volume    ------> Cluster level resource
			2) PVC: Persistent Volume Claim

Deployment----PVC----bound---PV----Storage-service---- 		
How to create PV using NFS Server
Type of access mode:
	1) ReadWriteOnce : At a one pod can use or mount
	2) ReadWriteMany : 
	3) ReadOnlyMany

persistentVolumeReclaimPolicy type:
    1) Recycle
	2) retain
	3) Deleted : will do cloud based .i.e. AWS
	
EBS : ReadWriteOnce
	
Create NFS server :
https://www.tecmint.com/how-to-setup-nfs-server-in-linux/	
	
##### https://kubernetes.io/docs/concepts/storage/persistent-volumes/	


PV ----> PVC----> Attach to Pod		
Create filder for configuration file:
		mkdir nfs
		cd nfs
		vi nfs-pv.yml
		cd..
		vim ./kube/config
		kubectl get clusterrole | grep admin    --------> check for the role
		kubectl create -f deployment-Based-pod.yml
		kubectl get pv
		kubectl create ns dev
		kubectl create -f PersistentVolumeClaim-pod.yml -n dev      ---> dev is new namesapece
		kubectl get pvc -n dev  ---> dev is new namesapece
	
How to use pvc in Pod
         kubectl create deploy mysql --image=mysql:5.6 -n dev --dry-run -o yaml > NFS-Deployment-mysql-pod.yml
		 vi NFS-Deployment-mysql-pod.yml
		 kubectl create -f NFS-Deployment-mysql-pod.yml -n dev
		 kubectl get po -n dev
		 kubectl logs <<pod-name>> -n dev
		 kubectl exec -it <pod name> /bin/bash
		 
		 kubectl edit pv pv1   -----> edit or encrase storage in PV

how to create localtype PV
		 
		 
---------------mount local volume--------------
	mkdir -p /volume/voll
	vim PersistentVolume-local.yml
	kubectl create -f PersistentVolume-local.yml    ---PV
	kubectl create -f PersistentVolumeClaim-local.yml  ---PVC
	cubectl get pvc
	kubectl create deploy mysql --image=nginx -n dev --dry-run -o yaml > local-Deployment-nginx-local-pod.yml
	kubectl create -f local-Deployment-nginx-local-pod.yml
	kubectl get po
	kubectl get po -o wide
	
---------------------------------------------(Day07)-----------------------------------------------------------------


	
---------------------------Quota-------------------

	1) Compute Quota: CPU/Memory
	   a) Pod level:
			kubectl create deploy mysql --image=nginx -n dev --dry-run -o yaml > Quota-Deployment-nginx-local-pod.yml
			vim Quota-Deployment-nginx-local-pod.yml
			kubectl create -f Quota-Deployment-nginx-local-pod.yml
			
	   
	   b) Deployment : Mysql : 4GB  / app: 2 GB                         1 CPU = 1000micron
	   c) Project level : 10GB RAM/2 CPU
	2) Resource Quota : pods/service/rs/deploy/pvc


------------Compute Quota: CPU/Memory--------------	
How to Apply compute Quota on deployment/ReplicaSet/RC
  kubectl get all
  kubeclt get po  
  kubectl get quota
  
  kubectl create quota mytest --hard=memory=1Gi,cpu=800m
  
  kubectl describe quota <quota name>
  kubectl get events | grep -i failed
  kubectl edit quota <quota name>    -------> Edit existing quota
  kubectl scale --replicas=4 deploy myapp
  watch Kubectl get po
  kubectl describe node <node name> | less
  
  kubectl create quota mytest --hard=memory=1Gi,cpu=800m -n qa    ----------> define quota for namesapece qa
  
------------Resource Quota : pods/service/rs/deploy/pvc--------------	  
How to apply Resource Quota  -----> its implemented on project level
   kubectl create quota mytest --hard=memory=1Gi,cpu=800m --dry-run -o yaml > Resource-Quota-Deployment-nginx-local-pod.yml
   vim Resource-Quota-Deployment-nginx-local-pod.yml
   kubectl apply -f Resource-Quota-Deployment-nginx-local-pod.yml
   kubectl get quota ResourceQuota
   kubectl get server  
   
How to create limit range :    -----> use to define limit rather on pod level
	vim limit-range-Deployment-nginx-local-pod.yml
	kubectl delete LimitRange	
	kubectl create -f limit-range-Deployment-nginx-local-pod.yml
	kubectl describe quota <quota name>
	
	
 	
   
   
----------------------Daemonsets--------------------
    
Pod Controller -----> ReplicaSet/ReplicaController                        -----> A DaemonSet ensures that all (or some) Nodes run a copy of a Pod
	kubectl create -f DaemonSet-Deployment-nginx-local-pod.yml
	kubectl get ds
	kubectl describe node master.example.com | grep -i taint
	kubeclt edit node worker1.example.com 
	
	
	
----------------------Types Pod--------------------	
	  regular pod
	  static pod    --> create static pod in any node  ---> there is no  benefit
			Create yaml file under the following path /etc/kubernetes/manifests      -----> Creating static pod



if etcd deleted from master.ecample.com 

Day08
-----------------------------------------multi Container Pod(Day08)-----------------------------------------------------------

 
	  
	
	



  
  
	
	
	
		
		
			
		
			 
-------------------------------------------------->Troubleshooting<--------------------------------------------------------			

systemctl status kubelet -l			

netstat -tnip | grep kube-proxy	   
kubectl get nodes
kubectl get po -o wide
kubectl delete po pod2 --force
kubectl get po -o wide

systemctl restart kubelet
 grep -f manifests /var/lib/kubeclt/config.yaml


vim .kube/config   ---> Change of configuration

kubectl describe node node1.example.com | less 

kubectl get events | grep -i failed


free -m

kubernetes certificate location /etc/kubernetes/pki/     ------> location for the certificate 

cd .kube/
Check for the application version /app/vi/serverresources.json | grep deployment
Check for the application version /vi/serverresources.json | grep service
			
list of kubernetes objects    ---> check google for objects 		

vim /etc/exports
			/exports/nfs *(rw, no_root_squash)
exportfs -r



	
	

fluentd ---log analytics tool




		
							
------------------------->Existing market technology REOKICASET<-----------------------------------------						
					
					
if you check the node for the pod then its under the docker
					docker ps | grep test <pod name>
					 
			 		
					 
Refrence :
https://v1-17.docs.kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/
https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/
https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/
https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/
					
					
			

